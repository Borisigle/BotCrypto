# Backtesting engine

The monitoring service now ships with a lightweight backtesting engine that replays
stored execution history to evaluate signal performance across rolling 30–90 day
windows. Analysts can surface the hit-rate, expectancy, and drawdown profile for
any combination of windows directly from the API or by using the bundled helper
script.

## Metrics captured

For each requested window the runner produces:

| Field | Description |
| --- | --- |
| `hit_rate` | Ratio of winning trades to total trades in the window. |
| `expectancy` | Expected return per trade (`avg_win * win_rate - avg_loss * loss_rate`). |
| `average_return` | Arithmetic mean of trade returns observed in the window. |
| `cumulative_return` | Multiplicative return across the window (`∏(1+return) - 1`). |
| `max_drawdown` | Largest peak-to-trough decline of the equity curve. |
| `meets_win_rate_threshold` | Highlights whether the window beats the configured/overridden win-rate target. |
| `sufficient_sample` | Indicates if a window satisfied the minimum trade count requirement. |

The summary section of the report mirrors the largest window requested, making it
straightforward to trend the broadest look-back period.

### Parameter overrides

The backtest runner accepts optional overrides so analysts can experiment with
alternative trade-classification rules:

- `win_return_threshold` – minimum fractional return required for a trade to count as a win.
- `loss_return_threshold` – maximum fractional return tolerated before a trade is treated as a loss.
- `min_trade_count` – minimum number of trades required before a window is considered statistically meaningful (defaults to 5).
- `min_win_rate` – target win rate used when flagging windows as passing/failing (defaults to the global threshold in `app/config.py`).

When overrides are omitted, the service relies on the stored execution outcomes
(`win` / `loss`) and global threshold configuration found in `app/config.py`.

## API usage

Two HTTP endpoints expose backtest results:

1. `GET /api/v1/backtests/report` – returns a JSON payload containing the per-window
   detail plus summary statistics.
2. `GET /api/v1/backtests/report/download` – streams a CSV export suitable for spreadsheet
   analysis. Parameters mirror the JSON endpoint.

Example request fetching 30, 60, and 90 day windows while enforcing a minimum win
rate of 50%:

```bash
curl "http://localhost:8080/api/v1/backtests/report?windows=30&windows=60&windows=90&min_win_rate=0.5"
```

To download the same data as CSV:

```bash
curl -L -o backtest.csv \
  "http://localhost:8080/api/v1/backtests/report/download?windows=45&windows=75&min_trade_count=10"
```

Both endpoints accept the optional override parameters listed above. Multiple
windows can be supplied by repeating the `windows` query argument.

## Logging

Each backtest execution is persisted to `app/data/backtest_logs/` for audit and
trend tracking:

- Individual run artefacts stored as timestamped JSON.
- A rolling `backtest_trend.csv` capturing the summary metrics for quick ad-hoc
  charts.

Set the `BACKTEST_LOG_PATH` environment variable to relocate the log directory if
persistent storage is available. The directory is ignored by Git and can be safely
mounted to durable storage in production deployments.

## Analyst script

A convenience CLI is available at `scripts/run_backtest.py` for analysts who prefer
command-line tools or ad-hoc experimentation without spinning up the API:

```bash
python scripts/run_backtest.py --windows 45,75 --min-win-rate 0.55 --csv reports/latest.csv
```

Arguments:

- `--snapshot` – path to a metrics snapshot (defaults to the service configuration).
- `--windows` – comma-separated window lengths in days (defaults to `30,60,90`).
- `--win-threshold` / `--loss-threshold` – return-based overrides for trade classification.
- `--min-trade-count` – minimum trades required before a window is flagged as a sufficient sample.
- `--min-win-rate` – win-rate target used when evaluating windows (0.0–1.0).
- `--csv` – optional path to write the CSV export alongside JSON printed to stdout.

The script prints the JSON report to stdout and writes a CSV summary when
requested. Logs generated by the script share the same location as API-triggered
runs, ensuring a unified view of backtest history.
